
def LSTM():
    
    print('Sentiment Analysis ì‹œì‘')

    import json
    import os
    from pprint import pprint
    from konlpy.tag import Okt
    import nltk
    import numpy as np
    import pandas as pd
    import time
    from keras.datasets import imdb
    from keras.models import Sequential
    from keras.layers import Dense, LSTM, Embedding
    from keras.preprocessing import sequence
    from keras.callbacks import EarlyStopping
    from keras.models import load_model
    from datetime import timedelta,date        
    
    def read_data(filename):
        with open(filename, 'r') as f:
            # tab ë³„ë¡œ ìë¥¸ë‹¤
            data = [line.split('\t') for line in f.read().splitlines()]
            # txt íŒŒì¼ì˜ í—¤ë”(id document label)ëŠ” ì œì™¸í•˜ê¸°
            data = data[1:]
        return data

    train_data = read_data('ratings_train.txt')
    test_data = read_data('ratings_test.txt')

    okt = Okt()

    def tokenize(doc):
        # í† í°ê³¼ ê·¼ì–´ ì‚¬ì´ì— '/'ë¡œ êµ¬ë¶€í•´ì¤ë‹ˆë‹¤
        # normì€ ì •ê·œí™”, stemì€ ê·¼ì–´ë¡œ í‘œì‹œí•˜ê¸°ë¥¼ ë‚˜íƒ€ëƒ„
        return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]

    # ë§¤ë²ˆ ë°˜ë³µí•˜ì§€ ì•Šê¸° ìœ„í•´ jsoníŒŒì¼ì´ ìˆìœ¼ë©´ ì½ì–´ì„œ ì‚¬ìš©
    if os.path.isfile('train_docs.json'):
        with open('train_docs.json') as f:
            train_docs = json.load(f)
        with open('test_docs.json') as f:
            test_docs = json.load(f)
    else:
        # row[1]ì— ë¦¬ë·°ê°€, row[2]ì— ë¶€ì •orê¸ì •ì´ ë‹´ê²¨ìˆìŒ
        train_docs = [(tokenize(row[1]), row[2]) for row in train_data]
        test_docs = [(tokenize(row[1]), row[2]) for row in test_data]
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open('train_docs.json', 'w', encoding="utf-8") as make_file:
            json.dump(train_docs, make_file, ensure_ascii=False, indent="\t")
        with open('test_docs.json', 'w', encoding="utf-8") as make_file:
            json.dump(test_docs, make_file, ensure_ascii=False, indent="\t")

    tokens = [t for d in train_docs for t in d[0]]

    text = nltk.Text(tokens, name='NMSC')



    # ëª¨ë“  ë¬¸ì¥ì„ í•™ìŠµí•  ìˆœ ì—†ìœ¼ë‹ˆ ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” 2500ê°œì˜ í† í°ì„ ì‚¬ìš©í•´ì„œ ë²¡í„°í™”
    # RAMì´ ë†’ë‹¤ë©´ 10000ê¹Œì§€ í•´ë´…ì‹œë‹¤
    selected_words = [f[0] for f in text.vocab().most_common(3000)]

    # selected_words ì•ˆì— ìˆëŠ” ë‹¨ì–´ë“¤ì´ docì•ˆì— ìˆëŠ”ì§€ í™•ì¸í•´ì„œ ë°˜í™˜
    # ë¬¸ì„œì§‘í•©ì—ì„œ ë‹¨ì–´ í† í°ì„ ìƒì„±í•˜ê³  ê° ë‹¨ì–´ì˜ ìˆ˜ë¥¼ ì„¸ì–´ BOW ì¸ì½”ë”©í•œ ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤
    # BOW(Back Of Words)
    def term_frequency(doc):
        return [doc.count(word) for word in selected_words]

    # token_list : 0 or 1(ê¸ì •,ë¶€ì •)ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ìˆìœ¼ë¯€ë¡œ token_listë§Œ í™•ì¸(d, _)
    # train_docs ì•ˆì— ìˆëŠ” toeknì¤‘ selected_wordsì— ë“¤ì–´ìˆëŠ” ë‹¨ì–´ë§Œ í¬í•¨
    # train_docsëŠ” 2ì°¨ì› listë“¤([[ì˜í™” ë¦¬ë·°], ê¸ì •orë¶€ì •])ë¡œ êµ¬ì„±ëœ 3ì°¨ì› list
    # train_xëŠ” 0ê³¼ 1ë¡œ ì´ë£¨ì–´ì§„ 5ì²œê°œì˜ listê°€ 15ë§Œê°œ ì¡´ì¬(2ì°¨ì› list)
    train_x = [term_frequency(d) for d, _ in train_docs]
    test_x = [term_frequency(d) for d, _ in test_docs]
    train_y = [c for _, c in train_docs]
    test_y = [c for _, c in test_docs]

    # ë°ì´í„°ê°€ ë¬¸ìì—´ì´ë‹ˆ inputì„ ìœ„í•´ floatìœ¼ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤
    # 15ë§Œê°œì˜ ë°ì´í„°ê°€ ê°ê° 2500ê°œì˜ 0ê³¼ 1ë¡œ ì¡´ì¬

    x_train = np.asarray(train_x).astype('float32')
    x_test = np.asarray(test_x).astype('float32')

    y_train = np.asarray(train_y).astype('float32')
    y_test = np.asarray(test_y).astype('float32')

    # LSTMì€ 3ì°¨ì› ë¦¬ìŠ¤íŠ¸ë§Œ inputìœ¼ë¡œ ë°›ìœ¼ë‹ˆ 3ì°¨ì›ìœ¼ë¡œ reshape í•´ì¤ë‹ˆë‹¤
    # [ìƒ˜í”Œ ìˆ˜, íƒ€ì„ìŠ¤í… ìˆ˜, ì†ì„± ìˆ˜]ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤
    # íƒ€ì„ìŠ¤í…ì´ë€ í•˜ë‚˜ì˜ ìƒ˜í”Œì— í¬í•¨ëœ ì‹œí€€ìŠ¤ ê°œìˆ˜(ì—¬ê¸°ì„  ë¦¬ë·° ê¸€ í•˜ë‚˜)
    # embedding ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ ë³€í™˜ë˜ì§€ë§Œ í•™ìŠµì´ ë„ˆë¬´ ëŠë ¤ì ¸ ì§ì ‘ ë³€í™˜í•´ì¤ë‹ˆë‹¤

    X_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))
    X_test = np.reshape(x_test, (x_test.shape[0], 1, x_train.shape[1]))


    model = Sequential()
    model.add(LSTM(100))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(loss='binary_crossentropy',
                optimizer='adam',
                metrics=['accuracy'])

    # ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê³¼ì í•©ì´ ì§„í–‰ë˜ë©´ ë°”ë¡œ EarlyStopping
    early_stopping = EarlyStopping()

    model.fit(X_train, y_train, validation_data=(X_test, y_test), 
            epochs=100, batch_size=25, callbacks=[early_stopping])

    scores = model.evaluate(X_test, y_test, verbose=0)

    model.save('Sentiment Analysis.h5')
    
    
    # ë¬¸ì¥ì—ì„œ ì´ëª¨í‹°ì½˜ ì²˜ë¦¬ë¥¼ í•´ì¤ë‹ˆë‹¤
    emoji = pd.read_csv('Emoji.csv')

    # emoji ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“¤ì–´ì„œ {emoji : emoji_sentiment}ë¡œ ë§Œë“¤ê¸°
    emoji_list = emoji['Emoji'].tolist()
    emoji_neg = emoji['Negative'].tolist()
    emoji_pos = emoji['Positive'].tolist()

    emoji.head(20)

    # ì´ëª¨í‹°ì½˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“¤ì–´ {ì´ëª¨í‹°ì½˜ : ê°ì •ë„} í˜•ì‹ìœ¼ë¡œ ì§ì„ ë§ì¶°ì¤ë‹ˆë‹¤
    emoji_dictionary = {}

    for i in range(len(emoji_list)):
        
        # ì´ëª¨í‹°ì½˜ ê¸ì •ì´ ë¶€ì •ë³´ë‹¤ ë†’ìœ¼ë©´ ì–‘ìˆ˜
        if(emoji_pos[i]-emoji_neg[i]>0):
            emoji_sentiment = emoji_pos[i]- emoji_neg[i]
        # ì´ëª¨í‹°ì½˜ ë¶€ì •ì´ ê¸ì •ë³´ë‹¤ ë†’ìœ¼ë©´ ìŒìˆ˜
        else:
            emoji_sentiment = emoji_pos[i]- emoji_neg[i]
        
        # ê°ê° scaleì´ ë‹¤ë¥´ë¯€ë¡œ ì†Œìˆ˜ì ìë¦¬ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤
        if(abs(emoji_sentiment)>=1000):
            emoji_sentiment/=1000000
        elif(abs(emoji_sentiment)>=100):
            emoji_sentiment/=100000
        elif(abs(emoji_sentiment)>=10):
            emoji_sentiment/=10000
        else:
            emoji_sentiment/=1000
        
        # ë”•ì…”ë„ˆë¦¬ì— {ì´ëª¨í‹°ì½˜ : ê°ì •ë„}ë¡œ ì¶”ê°€
        emoji_dictionary[emoji_list[i]] = emoji_sentiment

    def predict_sentiment_with_emoji(word):
        
        try:

            # ë¬¸ì¥ì„ tokení™” ì‹œí‚¤ê³ 
            token = tokenize(word)
            # ê°€ì¥ë§ì´ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì™€ í† í°ê³¼ ë¹„êµë¥¼í•˜ê³ 
            tf = term_frequency(token)
            # ë¬¸ì¥ì„ floatí˜•ì‹ìœ¼ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤
            data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)
            # LSTMì— ë§ê²Œ 3ì°¨ì› ë°°ì—´ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤
            data = np.reshape(data, (data.shape[0], 1, data.shape[1]))
            # ì˜ˆì „ì— í•™ìŠµí•œ LSTMëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤
            score = float(model.predict(data))
        
            # ë¬¸ì¥ì— ì´ëª¨í‹°ì½˜ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì´ëª¨í‹°ì½˜ì˜ sentimentë§Œí¼ ê°€ì¤‘ì¹˜ë¥¼ ì¤ë‹ˆë‹¤
            for emoji in emoji_dictionary:
                if emoji in word:
                    score += emoji_dictionary[emoji]
            
            # ì†Œìˆ˜ 5ìë¦¬ê¹Œì§€ë§Œ ë°˜í™˜
            return round(score, 5)
        
        except:
            
            return 0.5


    topics = ['society', 'politics', 'economic', 'foreign', 'culture',
            'entertain', 'sports', 'digital']

    today = int(date.today().strftime('%Y%m%d'))
    yesterday = date.today() - timedelta(1)
    yesterday = int(yesterday.strftime('%Y%m%d'))
    keywords = pd.read_csv('./Crawled Data/{}/{}_Top10_keyword'.format(today, today))
    keywords = keywords['Keyword'].values.tolist()

    n=10
    total_keyword_ranking10 = [keywords[i:i+n] for i in range(0, len(keywords), 10)]

    insta = pd.read_csv('./Crawled Data/{}/{}_instagram_dataframe'.format(today,today))
    daum_news = pd.read_csv('./Crawled Data/{}/{}_daum_news_dataframe'.format(today,today))
    twitter = pd.read_csv('./Crawled Data/{}/{}_tweet_dataframe'.format(today,today))
    youtube = pd.read_csv('./Crawled Data/{}/{}_youtube_dataframe'.format(today,today))

    # ë¹ˆ dataframeì„ ë§Œë“¤ì–´ì¤˜ì„œ ì°¨ê³¡ì°¨ê³¡ ë„£ëŠ”ë‹¤

    today_dataframe = pd.DataFrame(columns=['Topic', 'Keyword', 'Company', 'Title', 'Contents', 'Comments', 'KC', 'KCC'])

    # daum, youtube, insta, twitter ìˆœìœ¼ë¡œ ë„£ì–´ì¤€ë‹¤

    index = 0

    for i in range(80):
        today_dataframe.loc[index] = daum_news.loc[i]
        index += 1
        today_dataframe.loc[index] = youtube.loc[i]
        index += 1
        today_dataframe.loc[index] = insta.loc[i]
        index += 1
        today_dataframe.loc[index] = twitter.loc[i]
        index += 1


    # Keyword Total Ratio ì¶”ê°€
    # Total KTR, Topic KTR ë”°ë¡œ

    Total_KTR_list = []
    Topic_KTR_list = []
    Topic_KTR_dataframe = []
    KTR_list = []
    total_count_list = []
    Topic_mean_list = []

    total_count = 0

    # Total KTRì„ êµ¬í•˜ê¸° ìœ„í•´ í‰ê· ì„ êµ¬í•œë‹¤

    for i in range(0,320,4):
        count = 0
        for j in range(4):
            count += today_dataframe.iloc[i+j]['KC']
            count += today_dataframe.iloc[i+j]['KCC']
        total_count_list.append(count)
        total_count+= count
        
    mean_ratio = total_count/80

    # í‰ê· ëŒ€ë¹„ Keyword Total Ratio

    for i in range(80):
        total_KTR = round((total_count_list[i]/mean_ratio),2)
        Total_KTR_list.append(int(total_KTR*100))
        for j in range(4):
            KTR = round((total_count_list[i]/mean_ratio),2)
            KTR_list.append(str(int(KTR * 100))+'%')

    # Topicë³„ë¡œ KTRì„ êµ¬í•˜ê¸° ìœ„í•´ Topicë³„ë¡œ í‰ê· ì„ êµ¬í•œë‹¤

    for i in range(8):
        count=0
        for j in range(10):
            count+=total_count_list[10*i+j]
            
        Topic_mean_list.append(count/10)

        
    for i in range(8):
        for j in range(10):
            topic_ktr = round((total_count_list[10*i+j] / Topic_mean_list[i]),2)
            Topic_KTR_list.append(int(topic_ktr*100))

    time.sleep(5)

    print("ê°ì •ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤")

    # Keyword Total Sentiment ì¶”ê°€

    sentiment_list = []
    start_time = time.time()

    for i in range(320):
        
        title = str(today_dataframe.iloc[i]['Title'])
        content = str(today_dataframe.iloc[i]['Contents'])
        comment = str(today_dataframe.iloc[i]['Comments'])
        
        title_content_sentiment = predict_sentiment_with_emoji(title+content)
        comment_sentiment = predict_sentiment_with_emoji(comment)
        sentiment = round((title_content_sentiment + comment_sentiment)/2,2)
        sentiment_list.append(int(sentiment*100))

    print("ê±¸ë¦°ì‹œê°„ : {}ë¶„".format(round((time.time() - start_time)/60, 1)))
    print('\n')
    
    # Keyword Total Sentiment
    # Daum News 50%, Youtube 35%, Instagram 5%, Twitter 15%ì˜ ê°€ì¤‘ì¹˜ 

    today_dataframe_KTS = []
    Total_KTS_list = []

    for i in range(0,320,4):
        KTS = 0
        for j in range(4):
            if(j % 4==0):
                KTS += 0.5 * sentiment_list[i+j]
            elif(j % 4==1):
                KTS += 0.35 * sentiment_list[i+j]
            elif(j % 4==2):
                KTS += 0.05 * sentiment_list[i+j]
            else:
                KTS += 0.15 * sentiment_list[i+j]
        
        Total_KTS_list.append(int(KTS))

    topic_list = daum_news['Topic'].values.tolist()
    keyword_list = daum_news['Keyword'].values.tolist()

    today_KTR_KTS = pd.DataFrame({'Topic':topic_list,
                                    'Keyword':keyword_list,
                                    'Total_KTR':Total_KTR_list,
                                    'Topic_KTR':Topic_KTR_list,
                                    'KTS':Total_KTS_list})

    today_KTR_KTS.to_csv('./Crawled Data/{}/{}_KTR_KTS_dataframe'.format(today, today), index=False)
    


    # ì—°ê´€ê²€ìƒ‰ì–´ ê²€ìƒ‰
    keywords = pd.read_csv('./Crawled Data/{}/{}_Top10_keyword'.format(today, today))
    keyword_list = keywords['Keyword'].values.tolist()
    top5_topic = []

    # pytrendëŠ” keywordë¥¼ strì´ ì•„ë‹ˆë¼ listë¡œ ë°›ìœ¼ë¯€ë¡œ
    # top5 topicì„ ëª¨ë‘ 2ì°¨ì› listë¡œ ì„¤ì •

    for i in range(8):
        for j in range(10):
            top5_topic.append([keyword_list[10*i+j]])

    # top5 topicì˜ ì—°ê´€ê²€ìƒ‰ì–´ ì¶”ì¶œ

    print("ì—°ê´€ê²€ìƒ‰ì–´ ê²€ìƒ‰ ì‹œì‘")
    from pytrends.request import TrendReq

    top3_related_keyword = []
    top3_related_value = []

    pytrends = TrendReq(hl='ko')

    for i in range(len(top5_topic)):

        # ì‹œê°„ì„¤ì •ì€ ì§€ë‚œ í•˜ë£¨ë™ì•ˆ, ì§€ì—­ì€ í•œêµ­ì„¤ì •
        # 'rising'ë¶€ë¶„ì´ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œ ë¶€ë¶„ì…ë‹ˆë‹¤(ê°€ì¤‘ì¹˜ë„ í•¨ê»˜ ì¶œë ¥)

        try:
            pytrends.build_payload(top5_topic[i], geo = 'KR', timeframe='now 1-d')
            queries = pytrends.related_queries()
            dataframe = queries[top5_topic[i][0]]['rising']

            top3_related_keyword.append(dataframe['query'].values.tolist()[0:3])
            top3_related_value.append(dataframe['value'].values.tolist()[0:3])

        except:
            top3_related_keyword.append('[ì—†ìŒ]')
            top3_related_value.append('[ì—†ìŒ]')
            
    for i in range(80):
        
        if top3_related_keyword[i] == '[ì—†ìŒ]':
            top3_related_keyword[i] = '#ì—†ìŒ'
        else:
            for j in range(len(top3_related_keyword[i])):
                top3_related_keyword[i][j] = '#'+top3_related_keyword[i][j]
                if (" ") in top3_related_keyword[i][j]:
                    top3_related_keyword[i][j] = top3_related_keyword[i][j].replace(" ", "")


    for i in range(len(top3_related_keyword)):
        if(type(top3_related_keyword[i]) == list):
            top3_related_keyword[i] = (' ').join(top3_related_keyword[i])

    # today_KTR_KTS dataframe

    topic_list = daum_news['Topic'].values.tolist()
    keyword_list = daum_news['Keyword'].values.tolist()

    today_KTR_KTS = pd.DataFrame({'Topic':topic_list,
                                'Keyword':keyword_list,
                                'Total_KTR':Total_KTR_list,
                                'Topic_KTR':Topic_KTR_list,
                                'KTS':Total_KTS_list,
                                'Related_Keywords':top3_related_keyword})

    today_KTR_KTS.to_csv('./Crawled Data/{}/{}_KTR_KTS_dataframe'.format(today, today), index=False)

    # ì–´ì œì™€ ì˜¤ëŠ˜ ì¤‘ë³µëœ Keywordê°€ ìˆìœ¼ë©´ ë³€í™”ëŸ‰ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´
    # ì–´ì œì˜ csvíŒŒì¼ì„ ë¶ˆëŸ¬ì˜¨ë‹¤

    yesterday_KTR_KTS = pd.read_csv('./Crawled Data/{}/{}_KTR_KTS_dataframe'.format(yesterday, yesterday))

    compare_Total_KTR_list = Total_KTR_list
    compare_KTS_list = Total_KTS_list

    for i in range(8):

        for j in range(10):

            keyword = today_KTR_KTS.iloc[10*i+j]['Keyword']

            for k in range(10):

                if keyword == yesterday_KTR_KTS.iloc[10*i+k]['Keyword']:

                    Total_KTR_change = int(today_KTR_KTS.iloc[10*i+j]['Total_KTR']) - int(yesterday_KTR_KTS.iloc[10*i+k]['Total_KTR'])
                    KTR_change = str(abs(Total_KTR_change))
                    
                    if(Total_KTR_change < 0):
                        compare_Total_KTR_list[10*i+j] = (str(today_KTR_KTS.iloc[10*i+j]['Total_KTR'])+'% ('+KTR_change+'ğŸ”»)')
                    else:
                        compare_Total_KTR_list[10*i+j] = (str(today_KTR_KTS.iloc[10*i+j]['Total_KTR'])+'% ('+KTR_change+'ğŸ”º)')

                    Total_KTS_change = int(today_KTR_KTS.iloc[10*i+j]['KTS']) - int(yesterday_KTR_KTS.iloc[10*i+k]['KTS'])
                    KTS_change = str(abs(Total_KTS_change))
                    
                    if(Total_KTS_change < 0):
                        compare_KTS_list[10*i+j] = (str(today_KTR_KTS.iloc[10*i+j]['KTS'])+'% ('+KTS_change+'ğŸ”»)')
                    else:
                        compare_KTS_list[10*i+j] = (str(today_KTR_KTS.iloc[10*i+j]['KTS'])+'% ('+KTS_change+'ğŸ”º)')
    # %ë¥¼ ë¶™ì—¬ì¤ë‹ˆë‹¤
    for i in range(80):
        if ('%' not in str(compare_Total_KTR_list[i])):
            compare_Total_KTR_list[i] = str(compare_Total_KTR_list[i]) + '%'
        if ('%' not in str(compare_KTS_list[i])):
            compare_KTS_list[i] = str(compare_KTS_list[i]) + '%'

    

    today_KTR_KTS['Total_KTR_change'] = compare_Total_KTR_list
    today_KTR_KTS['KTS_change'] = compare_KTS_list

    today_KTR_KTS.to_csv('./Crawled Data/{}/{}_KTR_KTS_dataframe'.format(today, today), index=False)
